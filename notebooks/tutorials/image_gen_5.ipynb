{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0bf5ed0d1bfc6d19a0d1bba5b91c07a091be2c0f3786b839f216821a421533302",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "based on: https://medium.com/analytics-vidhya/transforming-the-world-into-paintings-with-cyclegan-6748c0b85632"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## The Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "data, metadata = tfds.load(\"cycle_gan/monet2photo\",\n",
    "    with_info=True, as_supervised=True)\n",
    "train_x, train_y = data[\"trainA\"], data[\"trainB\"]\n",
    "test_x, test_y = data[\"testA\"], data[\"testB\"]"
   ]
  },
  {
   "source": [
    "## Steup"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "\n",
    "# num training epochs\n",
    "EPOCHS = 50\n",
    "\n",
    "# relative importance of the cycle loss to the adversarial loss\n",
    "LAMBDA = 10\n",
    "\n",
    "img_rows, img_cols, channels = 256, 256, 3\n",
    "weight_initializer = RandomNormal(stddev=0.02)\n",
    "\n",
    "gen_g_optimizer = gen_f_optimizer = Adam(lr=0.0002, beta_1=0.5)\n",
    "dis_x_optimizer = dis_y_optimizer = Adam(lr=0.0002, beta_1=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "if tf.test.gpu_device_name():\n",
    "    print(f\"Default GPU Device: {tf.test.gpu_device_name()}\")\n",
    "    tf.device(tf.test.gpu_device_name())\n",
    "\n",
    "# TODO: add random cropping\n",
    "def preprocess_image(image, _):\n",
    "    return tf.reshape(\n",
    "        tf.cast(\n",
    "            tf.image.resize(image, (int(img_rows), int(img_cols))),\n",
    "            tf.float32\n",
    "        ) / 127.5 - 1,\n",
    "        (1, img_rows, img_cols, channels)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_x.map(preprocess_image)\n",
    "train_y = train_y.map(preprocess_image)\n",
    "test_x = test_x.map(preprocess_image)\n",
    "test_y = test_y.map(preprocess_image)"
   ]
  },
  {
   "source": [
    "## The Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, LeakyReLU\n",
    "from tensorflow_addons.layers import InstanceNormalization\n",
    "\n",
    "def Ck(inpt, k, use_instancenorm=True):\n",
    "    \"\"\"\n",
    "    Ck denotes a 4 × 4 Convolution-InstanceNorm-LeakyReLU layer \n",
    "    with k filters and stride 2.\n",
    "    \"\"\"\n",
    "    block = Conv2D(\n",
    "        k, \n",
    "        (4, 4), \n",
    "        strides=2, \n",
    "        padding=\"same\", \n",
    "        kernel_initializer=weight_initializer\n",
    "    )(inpt)\n",
    "\n",
    "    if use_instancenorm:\n",
    "        block = InstanceNormalization(axis=-1)(block)\n",
    "\n",
    "    block = LeakyReLU(0.2)(block)\n",
    "\n",
    "    return block\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Conv2D\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def discriminator():\n",
    "    dis_input = Input(shape=(img_rows, img_cols, channels))\n",
    "\n",
    "    d = Ck(dis_input, 64, False)\n",
    "    d = Ck(d, 128)\n",
    "    d = Ck(d, 256)\n",
    "    d = Ck(d, 512)\n",
    "\n",
    "    d = Conv2D(\n",
    "        1, \n",
    "        (4, 4), \n",
    "        padding=\"same\", \n",
    "        kernel_initializer=weight_initializer\n",
    "    )(d)\n",
    "\n",
    "    dis = Model(dis_input, d)\n",
    "    dis.compile(loss=\"mse\", optimizer=dis_x_optimizer)\n",
    "    return dis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Activation\n",
    "from tensorflow_addons.layers import InstanceNormalization\n",
    "\n",
    "def dk(k, use_instancenorm=True):\n",
    "    \"\"\"\n",
    "    dk denotes a 3×3 Convolution-InstanceNorm-ReLU with k \n",
    "    filters and stride 2.\n",
    "    \"\"\"\n",
    "    block = Sequential()\n",
    "    block.add(\n",
    "        Conv2D(\n",
    "            k,\n",
    "            (3, 3),\n",
    "            strides=2,\n",
    "            padding=\"same\",\n",
    "            kernel_initializer=weight_initializer\n",
    "        )\n",
    "    )\n",
    "\n",
    "    if use_instancenorm:\n",
    "        block.add(InstanceNormalization(axis=-1))\n",
    "    \n",
    "    block.add(Activation(\"relu\"))\n",
    "\n",
    "    return block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Activation\n",
    "from tensorflow_addons.layers import InstanceNormalization\n",
    "\n",
    "def uk(k):\n",
    "    \"\"\"\n",
    "    uk denotes a 3×3 fractional-strided-ConvolutionInstanceNorm-ReLU \n",
    "    layer with k filters and stride ½.\n",
    "    \"\"\"\n",
    "    block = Sequential()\n",
    "    block.add(\n",
    "        Conv2DTranspose(\n",
    "            k,\n",
    "            (3, 3),\n",
    "            strides=2,\n",
    "            padding=\"same\",\n",
    "            kernel_initializer=weight_initializer\n",
    "        )\n",
    "    )\n",
    "\n",
    "    block.add(InstanceNormalization(axis=-1))\n",
    "    block.add(Activation(\"relu\"))\n",
    "    \n",
    "    return block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Concatenate, Conv2DTranspose\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def generator():\n",
    "    gen_input = Input(shape=(img_rows, img_cols, channels))\n",
    "\n",
    "    encoder_layers = [\n",
    "        dk(64, False),\n",
    "        dk(128),\n",
    "        dk(256),\n",
    "        dk(512),\n",
    "        dk(512),\n",
    "        dk(512),\n",
    "        dk(512),\n",
    "        dk(512,)\n",
    "    ]\n",
    "\n",
    "    decoder_layers = [\n",
    "        uk(512),\n",
    "        uk(512),\n",
    "        uk(512),\n",
    "        uk(512),\n",
    "        uk(256),\n",
    "        uk(128),\n",
    "        uk(64),\n",
    "    ]\n",
    "\n",
    "    gen = gen_input\n",
    "\n",
    "    skips = []\n",
    "    for layer in encoder_layers:\n",
    "        gen = layer(gen)\n",
    "        skips.append(gen)\n",
    "\n",
    "    # reverse and remove first element\n",
    "    skips = skips[::-1][1:]\n",
    "\n",
    "    for skip_layer, layer in zip(skips, decoder_layers):\n",
    "        gen = layer(gen)\n",
    "        gen = Concatenate()([gen, skip_layer])\n",
    "\n",
    "    gen = Conv2DTranspose(\n",
    "        channels, \n",
    "        (3, 3),\n",
    "        strides=2,\n",
    "        padding=\"same\", \n",
    "        kernel_initializer=weight_initializer,\n",
    "        activation=\"tanh\"\n",
    "    )(gen)\n",
    "\n",
    "    return Model(gen_input, gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_g = generator()\n",
    "generator_f = generator()\n",
    "\n",
    "discriminator_x = discriminator()\n",
    "discriminator_y = discriminator()"
   ]
  },
  {
   "source": [
    "## Training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "\n",
    "loss = BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def discriminator_loss(real, generated):\n",
    "    return (loss(tf.ones_like(real), real) \n",
    "        + loss(tf.zeros_like(generated), generated)) * 0.5\n",
    "\n",
    "\n",
    "def gen_loss(validity):\n",
    "    return loss(tf.ones_like(validity), validity)\n",
    "\n",
    "\n",
    "def image_diff(image1, image2):\n",
    "    return tf.reduce_mean(tf.abs(image1 - image2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def step(real_x, real_y):\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        # compute discriminator y loss\n",
    "        fake_y = generator_g(real_x, training=True)\n",
    "        gen_g_validity = discriminator_y(fake_y, training=True)\n",
    "        dis_y_loss = discriminator_loss(\n",
    "            discriminator_y(real_y, training=True),\n",
    "            gen_g_validity\n",
    "        )\n",
    "\n",
    "        # compute and apply dicriminator y gradients\n",
    "        with tape.stop_recording():\n",
    "            discriminator_y_gradients = tape.gradient(\n",
    "                dis_y_loss,\n",
    "                discriminator_y.trainable_variables\n",
    "            )\n",
    "\n",
    "            dis_y_optimizer.apply_gradients(\n",
    "                zip(discriminator_y_gradients, discriminator_y.trainable_variables)\n",
    "            )\n",
    "\n",
    "        # compute discriminator x loss\n",
    "        fake_x = generator_f(real_y, training=True)\n",
    "        gen_f_validity = discriminator_x(fake_x, training=True)\n",
    "        dis_x_loss = discriminator_loss(\n",
    "            discriminator_x(real_x, training=True),\n",
    "            gen_f_validity\n",
    "        )\n",
    "\n",
    "        # compute and apply discriminator x gradients\n",
    "        with tape.stop_recording():\n",
    "            discriminator_x_gradients = tape.gradient(\n",
    "                dis_x_loss,\n",
    "                discriminator_x.trainable_variables\n",
    "            )\n",
    "\n",
    "            dis_x_optimizer.apply_gradients(\n",
    "                zip(discriminator_x_gradients, discriminator_x.trainable_variables)\n",
    "            )\n",
    "\n",
    "        # adversarial losses - how real the dis believed the generated images\n",
    "        gen_g_adv_loss = gen_loss(gen_g_validity)\n",
    "        gen_f_adv_loss = gen_loss(gen_f_validity)\n",
    "\n",
    "        # cycle losses - how well a translated image can be re-translated\n",
    "        cyc_x = generator_f(fake_y, training=True)\n",
    "        cyc_x_loss = image_diff(real_x, cyc_x)\n",
    "        cyc_y = generator_g(fake_x, training=True)\n",
    "        cyc_y_loss = image_diff(real_y, cyc_y)\n",
    "\n",
    "        # identity loss \n",
    "        id_x = generator_f(real_x, training=True)\n",
    "        id_x_loss = image_diff(real_x, id_x)\n",
    "        id_y = generator_g(real_y, training=True)\n",
    "        id_y_loss = image_diff(real_y, id_y)\n",
    "\n",
    "        # generator losses\n",
    "        gen_g_loss = gen_g_adv_loss + (cyc_x_loss + cyc_y_loss) * LAMBDA + id_y_loss * 0.5 * LAMBDA\n",
    "        gen_f_loss = gen_f_adv_loss + (cyc_x_loss + cyc_y_loss) * LAMBDA + id_x_loss * 0.5 * LAMBDA\n",
    "\n",
    "        # compute and apply generator gradients\n",
    "        with tape.stop_recording():\n",
    "            generator_g_gradients = tape.gradient(gen_g_loss, generator_g.trainable_variables)\n",
    "            gen_g_optimizer.apply_gradients(\n",
    "                zip(generator_g_gradients, generator_g.trainable_variables)\n",
    "            )\n",
    "\n",
    "            generator_f_gradients = tape.gradient(gen_f_loss, generator_f.trainable_variables)\n",
    "            gen_f_optimizer.apply_gradients(\n",
    "                zip(generator_f_gradients, generator_f.trainable_variables)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_images():\n",
    "    # sample images\n",
    "    x = next(iter(test_x.shuffle(1000))).numpy()\n",
    "    y = next(iter(test_y.shuffle(1000))).numpy()\n",
    "\n",
    "    # get predictions for those images\n",
    "    y_hat = generator_g.predict(x.reshape((1, img_rows, img_cols, channels)))\n",
    "    x_hat = generator_f.predict(y.reshape((1, img_rows, img_cols, channels)))\n",
    "\n",
    "    # plot images\n",
    "    plt.figure(figsize=(12, 12))\n",
    "\n",
    "    images = [x[0], y_hat[0], y[0], x_hat[0]]\n",
    "\n",
    "    for i in range(4):\n",
    "        plt.subplot(2, 2, i + 1)\n",
    "        plt.imshow(images[i] * 0.5 + 0.5)\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 0\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-dcfee535d223>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         step(\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_rows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_cols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_rows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_cols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1842\u001b[0m     \"\"\"\n\u001b[0;32m-> 1843\u001b[0;31m     return self._call_flat(\n\u001b[0m\u001b[1;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[1;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Epoch: {epoch}\")\n",
    "    start = time.time()\n",
    "\n",
    "    # Each batch\n",
    "    for k, (real_x, real_y) in enumerate(tf.data.Dataset.zip((train_x, train_y))):\n",
    "        if k % 100 == 0:\n",
    "            print(k)\n",
    "\n",
    "        step(\n",
    "            tf.reshape(real_x, (1, img_rows, img_cols, channels)), \n",
    "            tf.reshape(real_y, (1, img_rows, img_cols, channels))\n",
    "        )\n",
    "\n",
    "    generate_images()\n",
    "    print(f\"Time taken: {time.time() - start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}